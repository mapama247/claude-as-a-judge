# ü§ñ Claude-as-a-Judge ‚öñÔ∏è

## Set up
### Install required packages
```commandline
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
### Add your API credentials in `.env` file
```commandline
ANTHROPIC_API_KEY=sk-ant-api03-3bX4Q1TjinRYA4EAmX0VDGsGCHPDhChDn2TCC2wYsIQgj3z_blablabla
```

## How to Use
### Step 1) Generate responses

The only mandatory arguments are:
- `-i`: path to a `.jsonl` or `.txt` file with a list of prompts (one per line).
- `-m`: list with the two models to compare, either HuggingFace IDs or local paths.
```commandline
python generate_responses.py -i ./data/toy_prompts.txt -m google/gemma-2b google/gemma-2b-it
```
For fast debugging you can limit the number of examples adding the `-l` argument:
```commandline
python generate_responses.py -i ./data/toy_prompts.txt -l 2 -m ~/models/gemma-2b ~/models/gemma-2b-it
```

This will generate 3 output files:
- `responses_gemma-2b.jsonl`: Responses generated by the first model. Can be fed to [IFEval](https://github.com/google-research/google-research/tree/master/instruction_following_eval) (step 3).
- `responses_gemma-2b-it.jsonl`: Responses generated by the second model. Can be fed to [IFEval](https://github.com/google-research/google-research/tree/master/instruction_following_eval) (step 3).
- `responses_all.json`: Responses from both models stored in a single dictionary. Will be used in step 2.

These files will be saved in a `./output` directory, unless otherwise specified with the `-o` argument.
Inside that output directory there will be separate subdirectories for each dataset.

There are also arguments to control text generation parameters, such as *temperature* or *top_p*.

Check out the full list of available arguments with: `python generate_responses.py --help`.

Only activate `use_inference_client` if you don't have access to GPUs and want to run accelerated inference on HuggingFace‚Äôs infrastructure.
However, you'll be banned if you run too many consecutive requests, so only use this for small tests.

### Step 2) Pairwise comparison with Claude
Call the evaluation script giving it the JSON file generated in the previous step.
```commandline
python evaluate_with_claude.py -i ./output/toy_prompts/responses_all.json
```

The default judge is _claude-3-opus-20240229_, but other versions can be selected with the `--model_id` argument.

Again, the number of samples can be limited with `-l` and the output directory changed with `-o`. 

### Step 3) Evaluation with verifiable instructions (IFEval)
Clone the [official repository](https://github.com/google-research/google-research/tree/master/instruction_following_eval):
```commandline
git clone --depth 1 --filter=blob:none --sparse git@github.com:google-research/google-research.git && cd google-research && git sparse-checkout set instruction_following_eval
```
Install the required libraries in a new environment or the same as before.
```commandline
pip install -r requirements.txt
```
Download NLTK data if working in an offline cluster:
```commandline
python -m nltk.downloader all_data
```
Example of runner script:
```commandline
#!/bin/bash

source venv/bin/activate

NLTK_DATA=<LOCAL_PATH_TO_NLTK_DATA> # only needed in offline environment

RESPONSES_FILE=../output/ifeval_input_data/responses_gemma-2b-it.jsonl
OUTPUT_DIR=./output/gemma-2b-it

mkdir -p $OUTPUT_DIR

python3 evaluation_main.py \
  --input_data=./data/input_data.jsonl \
  --input_response_data=$RESPONSES_FILE \
  --output_dir=$OUTPUT_DIR

exit 0
```
